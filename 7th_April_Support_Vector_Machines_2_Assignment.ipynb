{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
        "\n",
        "In machine learning algorithms, particularly in Support Vector Machines (SVMs), the relationship between polynomial functions and kernel functions is significant. Kernel functions provide a way to implicitly transform input data into a higher-dimensional space, allowing the algorithm to capture more complex patterns that might not be linearly separable in the original feature space. Polynomial functions are a specific type of kernel function commonly used for this purpose.\n",
        "\n",
        "The relationship between polynomial functions and kernel functions lies in the fact that a polynomial kernel is a type of kernel function used in SVMs. The polynomial kernel allows the SVM to operate in a higher-dimensional space where it can find a hyperplane that separates classes in a non-linear fashion. The kernel function implicitly computes the dot product in this higher-dimensional space without explicitly transforming the input data."
      ],
      "metadata": {
        "id": "HvUoy44VaaY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
        "\n",
        "Scikit-learn provides the SVC (Support Vector Classification) class, and you can specify the polynomial kernel using the kernel parameter.\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "svm_poly = SVC(kernel='poly', degree=degree, C=C)\n"
      ],
      "metadata": {
        "id": "pW8UqHyHb0mW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
        "\n",
        "**Smaller Epsilon (Tight Tube):**\n",
        "- Results in a narrower epsilon-insensitive tube.\n",
        "- Aims for a more precise fit to the training data.\n",
        "- May lead to more support vectors.\n",
        "\n",
        "**Larger Epsilon (Wider Tube):**\n",
        "- Results in a wider epsilon-insensitive tube.\n",
        "- Allows for more errors within the tube without penalty.\n",
        "- May lead to fewer support vectors.\n",
        "\n",
        "**Summary:**\n",
        "- Smaller epsilon can lead to more support vectors, providing a precise fit.\n",
        "- Larger epsilon can lead to fewer support vectors, allowing a more relaxed fit.\n",
        "- The choice depends on the trade-off between precision and flexibility, often determined through cross-validation or grid search."
      ],
      "metadata": {
        "id": "G2Ah9qAsccHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
        "\n",
        "Support Vector Regression (SVR) has several key hyperparameters that significantly impact its performance. Here's an explanation of each parameter and how their choices affect SVR:\n",
        "\n",
        "1. **Kernel Function:**\n",
        "   - **Explanation:** The kernel function determines the type of mapping applied to the input space. Common choices include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
        "   - **Impact:** The kernel function influences the model's ability to capture complex relationships in the data. The choice depends on the nature of the underlying patterns.\n",
        "\n",
        "2. **C Parameter:**\n",
        "   - **Explanation:** The C parameter controls the trade-off between fitting the training data precisely and allowing for errors. A smaller C allows for a wider epsilon-insensitive tube (more errors are tolerated), while a larger C enforces a stricter fit to the training data.\n",
        "   - **Impact:**\n",
        "     - Smaller C: May lead to a more flexible model, potentially allowing for more errors.\n",
        "     - Larger C: Emphasizes fitting the training data more precisely, potentially leading to fewer errors.\n",
        "\n",
        "3. **Epsilon Parameter (ε):**\n",
        "   - **Explanation:** The epsilon parameter defines the width of the epsilon-insensitive tube. It determines the range within which errors are not penalized in the objective function.\n",
        "   - **Impact:**\n",
        "     - Smaller ε: Encourages a more precise fit to the training data, potentially leading to more support vectors.\n",
        "     - Larger ε: Allows for a more relaxed fit, potentially resulting in fewer support vectors.\n",
        "\n",
        "4. **Gamma Parameter:**\n",
        "   - **Explanation:** The gamma parameter influences the shape of the decision boundary. A higher gamma results in a more localized decision boundary, while a lower gamma leads to a more global decision boundary.\n",
        "   - **Impact:**\n",
        "     - Smaller gamma: Results in a smoother decision boundary, which may lead to underfitting.\n",
        "     - Larger gamma: Creates a more complex, localized decision boundary, which may lead to overfitting.\n",
        "\n",
        "**Examples of Parameter Tuning:**\n",
        "- **Kernel Function:**\n",
        "  - Use an RBF kernel when dealing with non-linear relationships.\n",
        "  - Use a linear kernel when the relationship between variables is approximately linear.\n",
        "\n",
        "- **C Parameter:**\n",
        "  - Increase C if you want a more precise fit to the training data.\n",
        "  - Decrease C to allow for more errors and obtain a more flexible model.\n",
        "\n",
        "- **Epsilon Parameter:**\n",
        "  - Decrease ε for a more precise fit, especially when the data has low noise.\n",
        "  - Increase ε for a more robust model that tolerates some errors.\n",
        "\n",
        "- **Gamma Parameter:**\n",
        "  - Increase gamma when the dataset is complex and has intricate patterns.\n",
        "  - Decrease gamma for smoother decision boundaries when dealing with simpler datasets.\n",
        "\n",
        "It's essential to perform cross-validation or grid search to find the best combination of hyperparameters for a given dataset. The optimal values depend on the specific characteristics of the data and the nature of the underlying relationships."
      ],
      "metadata": {
        "id": "JwQ7bgw6clrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. Assignment\n",
        "\n",
        ". Import the necessary libraries and load the dataset\n",
        "\n",
        ". Split the dataset into training and testing sets\n",
        "\n",
        ". Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
        "\n",
        ". Create an instance of the SVC classifier and train it on the training data\n",
        "\n",
        ". Use the trained classifier to predict the labels of the testing data\n",
        "\n",
        ". Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
        "precision, recall, F1-score)\n",
        "\n",
        ". Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to\n",
        "improve its performance\n",
        "\n",
        ". Train the tuned classifier on the entire dataset\n",
        "\n",
        ". Save the trained classifier to a file for future use.\n",
        "\n",
        "Note: You can use any dataset of your choice for this assignment, but make sure it is suitable for\n",
        "classification and has a sufficient number of features and samples."
      ],
      "metadata": {
        "id": "xmSjbvqzdQM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the data (Standard Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create an instance of the SVC classifier\n",
        "svm_classifier = SVC()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "svm_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Use the trained classifier to predict labels of the testing data\n",
        "y_pred = svm_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print('Classification Report:')\n",
        "print(classification_rep)\n",
        "\n",
        "# Tune the hyperparameters using GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['linear', 'rbf', 'poly']}\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best parameters from the grid search\n",
        "best_params = grid_search.best_params_\n",
        "print('Best Hyperparameters:', best_params)\n",
        "\n",
        "# Preprocess the entire dataset (Standard Scaling)\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train the tuned classifier on the entire dataset\n",
        "svm_classifier_tuned = SVC(**best_params)\n",
        "svm_classifier_tuned.fit(X_scaled, y)\n",
        "\n",
        "# Save the trained classifier to a file for future use\n",
        "joblib.dump(svm_classifier_tuned, 'svm_classifier_tuned.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq7uMdqkbzUW",
        "outputId": "33beef13-e262-455c-d9a6-68b1dfebd585"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Best Hyperparameters: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['svm_classifier_tuned.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}
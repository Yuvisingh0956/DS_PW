{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "Let's use conditional probability to find the probability that an employee is a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "Let:\n",
    "- \\( A \\) be the event that an employee uses the health insurance plan.\n",
    "- \\( B \\) be the event that an employee is a smoker.\n",
    "\n",
    "The probability that an employee uses the health insurance plan is \\( P(A) = 0.70 \\) (given as 70%).\n",
    "\n",
    "The probability that an employee who uses the health insurance plan is a smoker is \\( P(B|A) = 0.40 \\) (given as 40%).\n",
    "\n",
    "The probability of an employee being a smoker given that he/she uses the health insurance plan, denoted as \\( P(B|A) \\), is calculated using the formula:\n",
    "\n",
    "\\[ P(B|A) = \\frac{P(A \\cap B)}{P(A)} \\]\n",
    "\n",
    "In this case, \\( P(A \\cap B) \\) is the probability that an employee both uses the health insurance plan and is a smoker.\n",
    "\n",
    "\\[ P(B|A) = \\frac{P(A \\cap B)}{P(A)} \\]\n",
    "\n",
    "\\[ P(B|A) = 0.4/0.7 \\]\n",
    "\n",
    "Now, calculate:\n",
    "\n",
    "\\[ P(B|A) = 4/7 \\]\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 4/7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "The key difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the nature of the features and the underlying assumptions about the data.\n",
    "\n",
    "1. **Bernoulli Naive Bayes:**\n",
    "   - **Feature Type:** It is suitable for binary feature data, where features can take on only two values (typically 0 and 1).\n",
    "   - **Example Use Cases:** Text classification where features represent the presence (1) or absence (0) of certain words in a document. It's often used in document classification tasks.\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "   - **Feature Type:** It is designed for discrete data, specifically for features that represent counts or frequencies (e.g., word counts in a document).\n",
    "   - **Example Use Cases:** Text classification where features are word frequencies in a document. It is commonly used in natural language processing tasks such as spam filtering, sentiment analysis, and topic classification.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is suitable for binary feature data, while Multinomial Naive Bayes is designed for discrete feature data, typically representing counts or frequencies. The choice between them depends on the nature of the data you are working with and the assumptions that align with your specific classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Bernoulli Naive Bayes assumes binary feature data, where features can take on values of 0 or 1. When dealing with missing values in Bernoulli Naive Bayes, there are a few common approaches:\n",
    "\n",
    "1. **Imputation with Zero or One:**\n",
    "   - If a missing value is encountered, it can be imputed (filled in) with either 0 or 1, depending on the context and the nature of the data. This assumes that the missing value can be reasonably considered as the absence (0) or presence (1) of a certain feature.\n",
    "\n",
    "2. **Ignoring Missing Values:**\n",
    "   - Another approach is to simply ignore instances with missing values during the training and classification steps. This means that instances with missing values won't contribute to the calculation of probabilities for the corresponding features.\n",
    "\n",
    "3. **Imputation with Feature Statistics:**\n",
    "   - Missing values can be imputed based on the statistics of the observed values for the specific feature. For example, you might replace missing values with the mean or mode of the observed values for that feature.\n",
    "\n",
    "It's important to note that the choice of how to handle missing values depends on the nature of the data and the specific characteristics of the problem at hand. The chosen approach should be reasonable and aligned with the assumptions of the Bernoulli Naive Bayes model for binary feature data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. The Gaussian Naive Bayes algorithm is an extension of the Naive Bayes algorithm for situations where the features are continuous and assumed to follow a Gaussian (normal) distribution. While it's commonly used for binary classification problems, it can be adapted for multi-class classification as well.\n",
    "\n",
    "In the context of multi-class classification, the Gaussian Naive Bayes algorithm operates by estimating the parameters of the Gaussian distribution (mean and variance) for each class and each feature. When making predictions for a new instance, it calculates the probability of the instance belonging to each class based on the Gaussian distribution parameters and then assigns the class with the highest probability.\n",
    "\n",
    "In summary, Gaussian Naive Bayes can be applied to handle multi-class classification problems, making it a versatile algorithm for scenarios where features are continuous and assumed to be normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8856762237102707\n",
      "Precision: 0.8855034508139028\n",
      "Recall: 0.8157853196527229\n",
      "F1 Score: 0.8490373364494272\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7902612468169388\n",
      "Precision: 0.7406952001397835\n",
      "Recall: 0.7214801772812822\n",
      "F1 Score: 0.7305907760768595\n",
      "\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8202555880411204\n",
      "Precision: 0.698905668336373\n",
      "Recall: 0.9575344544957805\n",
      "F1 Score: 0.8078451270743713\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Load the Spambase dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "names = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\", \"word_freq_over\",\n",
    "         \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\",\n",
    "         \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \"word_freq_free\",\n",
    "         \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\", \"word_freq_your\",\n",
    "         \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\",\n",
    "         \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\",\n",
    "         \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\",\n",
    "         \"word_freq_pm\", \"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\",\n",
    "         \"word_freq_project\", \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\",\n",
    "         \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \"capital_run_length_average\",\n",
    "         \"capital_run_length_longest\", \"capital_run_length_total\", \"spam_class\"]\n",
    "data = pd.read_csv(url, names=names, delimiter=\",\")\n",
    "X = data.drop(\"spam_class\", axis=1)\n",
    "y = data[\"spam_class\"]\n",
    "\n",
    "# Define classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Define performance metrics\n",
    "scoring_metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate classifiers using cross-validation\n",
    "bernoulli_scores = cross_validate(bernoulli_nb, X, y, cv=cv, scoring=scoring_metrics)\n",
    "multinomial_scores = cross_validate(multinomial_nb, X, y, cv=cv, scoring=scoring_metrics)\n",
    "gaussian_scores = cross_validate(gaussian_nb, X, y, cv=cv, scoring=scoring_metrics)\n",
    "\n",
    "# Display results\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy:\", bernoulli_scores['test_accuracy'].mean())\n",
    "print(\"Precision:\", bernoulli_scores['test_precision'].mean())\n",
    "print(\"Recall:\", bernoulli_scores['test_recall'].mean())\n",
    "print(\"F1 Score:\", bernoulli_scores['test_f1'].mean())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "print(\"Accuracy:\", multinomial_scores['test_accuracy'].mean())\n",
    "print(\"Precision:\", multinomial_scores['test_precision'].mean())\n",
    "print(\"Recall:\", multinomial_scores['test_recall'].mean())\n",
    "print(\"F1 Score:\", multinomial_scores['test_f1'].mean())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(\"Accuracy:\", gaussian_scores['test_accuracy'].mean())\n",
    "print(\"Precision:\", gaussian_scores['test_precision'].mean())\n",
    "print(\"Recall:\", gaussian_scores['test_recall'].mean())\n",
    "print(\"F1 Score:\", gaussian_scores['test_f1'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "- **Best Performance:** Bernoulli Naive Bayes achieved the highest accuracy, precision, and F1 score among the three variants. It performed well in capturing both true positives and true negatives.\n",
    "  \n",
    "- **Multinomial Naive Bayes:** While it showed reasonable performance, it performed slightly lower than Bernoulli Naive Bayes. It's worth noting that the Multinomial variant is designed for discrete data and may not be as well-suited for this dataset with binary features.\n",
    "\n",
    "- **Gaussian Naive Bayes:** While having a high recall, indicating it identifies a high proportion of actual spams, it has lower precision. This means it may classify some non-spam instances as spam. The Gaussian variant assumes a normal distribution for features, which might not be an ideal assumption for this dataset.\n",
    "\n",
    "**Limitations of Naive Bayes:**\n",
    "- **Assumption of Independence:** Naive Bayes assumes that features are independent, which might not hold true in real-world scenarios.\n",
    "  \n",
    "- **Sensitive to Feature Correlation:** If features are highly correlated, it can negatively impact the performance of Naive Bayes.\n",
    "\n",
    "- **Limited Expressiveness:** Naive Bayes may not capture complex relationships in the data, especially when interactions between features are essential for accurate predictions.\n",
    "\n",
    "In conclusion, the choice of the best variant depends on the specific characteristics of the dataset. In this case, Bernoulli Naive Bayes performed the best overall, likely because the dataset contains binary features. Understanding the nature of the features and the underlying assumptions of each variant is crucial for selecting an appropriate Naive Bayes model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e740366d-de0e-40f7-8461-db86e05dae99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               pickup             dropoff  passengers  distance  fare   tip  \\\n",
      "0 2019-03-23 20:21:09 2019-03-23 20:27:24           1      1.60   7.0  2.15   \n",
      "1 2019-03-04 16:11:55 2019-03-04 16:19:00           1      0.79   5.0  0.00   \n",
      "2 2019-03-27 17:53:01 2019-03-27 18:00:25           1      1.37   7.5  2.36   \n",
      "3 2019-03-10 01:23:59 2019-03-10 01:49:51           1      7.70  27.0  6.15   \n",
      "4 2019-03-30 13:27:42 2019-03-30 13:37:14           3      2.16   9.0  1.10   \n",
      "\n",
      "   tolls  total   color      payment            pickup_zone  \\\n",
      "0    0.0  12.95  yellow  credit card        Lenox Hill West   \n",
      "1    0.0   9.30  yellow         cash  Upper West Side South   \n",
      "2    0.0  14.16  yellow  credit card          Alphabet City   \n",
      "3    0.0  36.95  yellow  credit card              Hudson Sq   \n",
      "4    0.0  13.40  yellow  credit card           Midtown East   \n",
      "\n",
      "            dropoff_zone pickup_borough dropoff_borough  \n",
      "0    UN/Turtle Bay South      Manhattan       Manhattan  \n",
      "1  Upper West Side South      Manhattan       Manhattan  \n",
      "2           West Village      Manhattan       Manhattan  \n",
      "3         Yorkville West      Manhattan       Manhattan  \n",
      "4         Yorkville West      Manhattan       Manhattan  \n",
      "\n",
      " \n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>fare</th>\n",
       "      <th>tip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043597</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.064759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021526</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037330</td>\n",
       "      <td>0.043624</td>\n",
       "      <td>0.071084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.209809</td>\n",
       "      <td>0.174497</td>\n",
       "      <td>0.185241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.058856</td>\n",
       "      <td>0.053691</td>\n",
       "      <td>0.033133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6428</th>\n",
       "      <td>0.020436</td>\n",
       "      <td>0.023490</td>\n",
       "      <td>0.031928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6429</th>\n",
       "      <td>0.510627</td>\n",
       "      <td>0.382550</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6430</th>\n",
       "      <td>0.112807</td>\n",
       "      <td>0.100671</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6431</th>\n",
       "      <td>0.030518</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6432</th>\n",
       "      <td>0.104905</td>\n",
       "      <td>0.093960</td>\n",
       "      <td>0.101205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6433 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      distance      fare       tip\n",
       "0     0.043597  0.040268  0.064759\n",
       "1     0.021526  0.026846  0.000000\n",
       "2     0.037330  0.043624  0.071084\n",
       "3     0.209809  0.174497  0.185241\n",
       "4     0.058856  0.053691  0.033133\n",
       "...        ...       ...       ...\n",
       "6428  0.020436  0.023490  0.031928\n",
       "6429  0.510627  0.382550  0.000000\n",
       "6430  0.112807  0.100671  0.000000\n",
       "6431  0.030518  0.033557  0.000000\n",
       "6432  0.104905  0.093960  0.101205\n",
       "\n",
       "[6433 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "'''\n",
    "\n",
    "Min-max scaling is a data preprocessing technique that transforms the values of each feature in a dataset to a common scale, typically between 0 and 1. \n",
    "This is done by subtracting the minimum value of each feature from each value and then dividing by the difference between the maximum and minimum values.\n",
    "\n",
    "Min-max scaling is used to normalize the data so that all features have the same range of values. \n",
    "This can be helpful for machine learning algorithms, as it can improve the performance of the algorithm and make the results more interpretable.\n",
    "'''\n",
    "\n",
    "\"Example of min max scaling in taxis dataset: \"\n",
    "\n",
    "import seaborn as sns\n",
    "df = sns.load_dataset('taxis')\n",
    "print(df.head())\n",
    "print(\"\\n \\n \\n\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "min_max = MinMaxScaler()\n",
    "pd.DataFrame(min_max.fit_transform(df[['distance', 'fare', 'tip']]), columns = ['distance', 'fare', 'tip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeebfd54-5700-4536-9c37-6a5e12163732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n",
      "\n",
      " \n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.803773</td>\n",
       "      <td>0.551609</td>\n",
       "      <td>0.220644</td>\n",
       "      <td>0.031521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.828133</td>\n",
       "      <td>0.507020</td>\n",
       "      <td>0.236609</td>\n",
       "      <td>0.033801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.805333</td>\n",
       "      <td>0.548312</td>\n",
       "      <td>0.222752</td>\n",
       "      <td>0.034269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.800030</td>\n",
       "      <td>0.539151</td>\n",
       "      <td>0.260879</td>\n",
       "      <td>0.034784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.790965</td>\n",
       "      <td>0.569495</td>\n",
       "      <td>0.221470</td>\n",
       "      <td>0.031639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.721557</td>\n",
       "      <td>0.323085</td>\n",
       "      <td>0.560015</td>\n",
       "      <td>0.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.729654</td>\n",
       "      <td>0.289545</td>\n",
       "      <td>0.579090</td>\n",
       "      <td>0.220054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.716539</td>\n",
       "      <td>0.330710</td>\n",
       "      <td>0.573231</td>\n",
       "      <td>0.220474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.674671</td>\n",
       "      <td>0.369981</td>\n",
       "      <td>0.587616</td>\n",
       "      <td>0.250281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.690259</td>\n",
       "      <td>0.350979</td>\n",
       "      <td>0.596665</td>\n",
       "      <td>0.210588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width\n",
       "0        0.803773     0.551609      0.220644     0.031521\n",
       "1        0.828133     0.507020      0.236609     0.033801\n",
       "2        0.805333     0.548312      0.222752     0.034269\n",
       "3        0.800030     0.539151      0.260879     0.034784\n",
       "4        0.790965     0.569495      0.221470     0.031639\n",
       "..            ...          ...           ...          ...\n",
       "145      0.721557     0.323085      0.560015     0.247699\n",
       "146      0.729654     0.289545      0.579090     0.220054\n",
       "147      0.716539     0.330710      0.573231     0.220474\n",
       "148      0.674671     0.369981      0.587616     0.250281\n",
       "149      0.690259     0.350979      0.596665     0.210588\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "#Provide an example to illustrate its application.\n",
    "'''\n",
    "The Unit Vector technique in feature scaling is a method of scaling the features in a dataset so that they all have a length of 1. \n",
    "This is done by dividing each feature by its Euclidean norm.\n",
    "\n",
    "The Unit Vector technique is different from Min-Max scaling in that it does not scale the features to a common range. \n",
    "Instead, it scales the features so that they all have a length of 1.\n",
    "'''\n",
    "\n",
    "\"Example for Unit vector technique: \"\n",
    "\n",
    "import seaborn as sns\n",
    "df = sns.load_dataset('iris')\n",
    "print(df.head())\n",
    "print(\"\\n \\n \\n\")\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(normalize(df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]), \n",
    "             columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5736e07-d947-438c-8a79-19cf4cc28447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "#example to illustrate its application.\n",
    "'''\n",
    "Principal component analysis (PCA) is a statistical procedure that is used to reduce the dimensionality of a dataset. \n",
    "PCA works by finding a set of orthogonal axes, called principal components, that capture the most variance in the data. \n",
    "The number of principal components is typically much smaller than the number of original features, \n",
    "so PCA can be used to reduce the size of the dataset while still preserving most of the information.\n",
    "\n",
    "Here is an example of how PCA can be used to reduce the dimensionality of a dataset. \n",
    "Consider a dataset of student grades on a test. The dataset contains 100 students and 5 test scores. \n",
    "The original dataset can be represented as a 100x5 matrix, where each row represents a student and each column represents a test score.\n",
    "\n",
    "PCA can be used to reduce the dimensionality of this dataset to 2 dimensions. \n",
    "To do this, we would first calculate the covariance matrix of the dataset. The covariance matrix is a square matrix \n",
    "that stores the covariance between each pair of features.\n",
    "\n",
    "Once we have calculated the covariance matrix, we can find the eigenvectors and eigenvalues of the matrix. \n",
    "The eigenvectors are the principal components, and the eigenvalues are the variances of the principal components.\n",
    "\n",
    "The first principal component is the eigenvector with the largest eigenvalue. \n",
    "The second principal component is the eigenvector with the second largest eigenvalue, and so on.\n",
    "\n",
    "We can then project the original dataset onto the first two principal components. \n",
    "This will give us a 100x2 matrix, where each row represents a student and each column represents a principal component.\n",
    "\n",
    "The first principal component will capture the most variance in the data, and the second principal component will capture the \n",
    "second most variance. The remaining principal components will capture less variance, and can be discarded.\n",
    "\n",
    "In this example, we have reduced the dimensionality of the original dataset from 5 dimensions to 2 dimensions. \n",
    "This can be helpful for visualization and machine learning.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d3dc2-ac47-4fdb-b594-9cac4d42b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? \n",
    "#Provide an example to illustrate this concept.\n",
    "'''\n",
    "Feature extraction is a process of transforming raw data into features that are more relevant for a particular task. \n",
    "PCA can be used for feature extraction by selecting a subset of principal components that are most relevant for the task.\n",
    "\n",
    "Here is an example of how PCA can be used for feature extraction.\n",
    "Consider a dataset of images of handwritten digits. The dataset contains 10,000 images, each of which is a 28x28 pixel image \n",
    "of a handwritten digit. The original dataset can be represented as a 10,000x784 matrix, \n",
    "where each row represents an image and each column represents a pixel.\n",
    "\n",
    "PCA can be used to reduce the dimensionality of this dataset to 2 dimensions.\n",
    "\n",
    "In this example, we have reduced the dimensionality of the original dataset from 784 dimensions to 2 dimensions. \n",
    "This can be helpful for visualization and machine learning.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6edfe-c64e-4492-9fa9-2f24454baa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "#contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "#preprocess the data.\n",
    "'''\n",
    "In the context of a food delivery service, we could use Min-Max scaling to normalize the price, rating, and delivery time \n",
    "features. This would make it easier for the recommendation system to learn the relationship between these features\n",
    "and the user's preferences.\n",
    "\n",
    "For example, let's say that the price of a meal ranges from Rs. 5 to Rs. 50, the rating ranges from 1 to 5, \n",
    "and the delivery time ranges from 15 minutes to 60 minutes. \n",
    "We could use Min-Max scaling to transform these features to a range of 0 to 1 as follows:\n",
    "\n",
    "Price | Min-Max scaled value\n",
    "$5 | 0\n",
    "$10 | 0.2\n",
    "$15 | 0.4\n",
    "$50 | 1\n",
    "\n",
    "Rating | Min-Max scaled value\n",
    "1 | 0\n",
    "2 | 0.2\n",
    "3 | 0.4\n",
    "5 | 1\n",
    "\n",
    "Delivery time | Min-Max scaled value\n",
    "15 minutes | 0\n",
    "30 minutes | 0.5\n",
    "45 minutes | 1\n",
    "\n",
    "Once the features have been normalized, the recommendation system can learn the relationship between these features \n",
    "and the user's preferences. \n",
    "This will allow the recommendation system to recommend meals that are likely to be of interest to the user.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161a247-1688-45f2-b788-dfef3af5eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, \n",
    "# such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "'''\n",
    "In the context of stock price prediction, we could use PCA to reduce the dimensionality of the dataset by selecting a subset of principal components \n",
    "that are most relevant for predicting stock prices. For example, we could select the principal components that capture the most variance in the company \n",
    "financial data and market trends. This would reduce the size of the dataset without losing too much information, which could improve the performance of the model.\n",
    "\n",
    "Here are the steps on how to use PCA to reduce the dimensionality of a dataset for stock price prediction:\n",
    "\n",
    "1. Standardize the features: This is done by subtracting the mean of each feature from each value and then dividing by the standard deviation of the feature.\n",
    "2. Calculate the covariance matrix: The covariance matrix is a square matrix that stores the covariance between each pair of features.\n",
    "3. Find the eigenvectors and eigenvalues of the covariance matrix: The eigenvectors are the principal components, and the eigenvalues are the variances of the principal components.\n",
    "4. Sort the eigenvalues in descending order.\n",
    "5. Select the top k principal components, where k is the desired number of dimensions.\n",
    "6. Project the original dataset onto the selected principal components.\n",
    "\n",
    "The resulting dataset will have a reduced dimensionality while still preserving most of the information. \n",
    "This dataset can then be used to train a model to predict stock prices.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74989d71-e921-4dce-8303-e1b41576c38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "#values to a range of -1 to 1.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data = [[1], [5], [10], [15], [20]]\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515f5aa-488e-42b5-a1df-39043baa80fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "# Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "'''\n",
    "To determine how many principal components to retain during feature extraction using PCA (Principal Component Analysis), \n",
    "it is necessary to consider the explained variance ratio associated with each principal component. \n",
    "The explained variance ratio indicates the proportion of the dataset's variance that is captured by each principal component.\n",
    "A higher explained variance ratio implies that the principal component retains more information from the original dataset.\n",
    "\n",
    "Here's how you can approach the task of determining the number of principal components to retain using PCA:\n",
    "\n",
    "1. Standardize the dataset\n",
    "2. Apply PCA and calculate explained variance\n",
    "3. Evaluate the cumulative explained variance\n",
    "4. Choose the number of principal components\n",
    "\n",
    "It's important to note that the number of principal components to retain is specific to the dataset and the specific goals of the analysis. \n",
    "Different datasets may require different numbers of principal components to capture sufficient variance.\n",
    "\n",
    "In summary, the number of principal components to retain during feature extraction using PCA should be determined based on the explained \n",
    "variance ratios and the desired level of variance retention for the specific dataset and analysis goals.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

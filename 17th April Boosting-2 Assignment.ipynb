{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Gradient Boosting Regression is a machine learning technique used for regression problems. It belongs to the family of boosting algorithms, specifically gradient boosting machines, which sequentially build an ensemble of weak learners (usually decision trees) to create a strong predictive model. Gradient Boosting Regression minimizes a loss function by adding weak learners to the ensemble in a way that corrects the errors of the existing ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 208.0649\n",
      "R-squared: -1.4958\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a small dataset for regression\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 2\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the gradient boosting regressor class\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions with the mean of the target variable\n",
    "        predictions = np.full_like(y, np.mean(y))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate residuals\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Train a decision tree on residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=3)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update predictions using the learning rate and predictions from the tree\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "            # Store the trained tree in the ensemble\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using the ensemble of trees\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for tree in self.models:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Instantiate the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'R-squared: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "Mean Squared Error: 2.8254\n",
      "R-squared: 0.9661\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a small dataset for regression\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 2\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model with different hyperparameter combinations\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Make predictions using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'R-squared: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "In the context of Gradient Boosting, a weak learner refers to a model or hypothesis that performs only slightly better than random chance on a given learning task. Specifically, weak learners are simple models that may have limited complexity and predictive power individually. Despite their simplicity, weak learners can be combined in an ensemble to create a strong predictive model through the boosting process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm lies in the sequential building of an ensemble of weak learners to create a strong and accurate predictive model. The key ideas and intuition behind Gradient Boosting are as follows:\n",
    "\n",
    "1. **Sequential Correction of Errors:**\n",
    "   - Gradient Boosting works in an iterative manner, focusing on correcting the errors made by the existing ensemble in each iteration.\n",
    "   - At each step, a new weak learner is added to the ensemble, and it is trained to capture the mistakes or residuals of the current ensemble.\n",
    "\n",
    "2. **Gradient Descent Optimization:**\n",
    "   - The algorithm uses gradient descent optimization to minimize a loss function. The loss function quantifies the difference between the model's predictions and the actual target values.\n",
    "   - In each iteration, the weak learner is trained to approximate the negative gradient of the loss function with respect to the current predictions.\n",
    "\n",
    "3. **Adaptive Learning:**\n",
    "   - The learning process is adaptive, with each new weak learner adjusting its predictions based on the errors of the existing ensemble.\n",
    "   - Instances that are difficult to predict correctly are assigned higher weights, and subsequent weak learners pay more attention to these challenging instances.\n",
    "\n",
    "4. **Combination of Weak Learners:**\n",
    "   - The final prediction is the sum of the predictions made by all weak learners in the ensemble, each weighted by a factor that reflects its contribution to reducing the overall loss.\n",
    "   - By combining the predictions of weak learners, the ensemble creates a robust model capable of capturing complex relationships in the data.\n",
    "\n",
    "5. **Flexibility and Model Capacity:**\n",
    "   - Gradient Boosting is flexible and can accommodate various types of weak learners, such as decision trees. The simplicity of individual weak learners makes the algorithm less prone to overfitting.\n",
    "   - The algorithm's capacity to fit the training data improves with each iteration, allowing it to adapt to complex patterns and outliers.\n",
    "\n",
    "6. **Regularization and Hyperparameters:**\n",
    "   - Hyperparameters such as the learning rate, the number of trees, and tree depth play a crucial role in controlling the behavior of the algorithm and preventing overfitting.\n",
    "   - Regularization techniques, such as shrinkage and tree pruning, contribute to the model's generalization ability.\n",
    "\n",
    "In summary, Gradient Boosting builds a powerful ensemble model by iteratively improving the predictions of weak learners, with each learner focusing on the errors of the existing ensemble. The algorithm's adaptability, sequential correction of errors, and use of gradient descent optimization contribute to its effectiveness in creating accurate and robust predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners sequentially to create a strong predictive model. The process involves the iterative addition of weak learners, each focusing on correcting the errors made by the existing ensemble. Here is a step-by-step explanation of how Gradient Boosting builds the ensemble:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - The ensemble starts with an initial prediction, often the mean of the target variable for regression problems or a class probability for classification problems.\n",
    "\n",
    "2. **Compute Residuals:**\n",
    "   - Compute the residuals, which are the differences between the actual target values and the current predictions. Residuals represent the errors made by the current ensemble.\n",
    "\n",
    "3. **Train Weak Learner:**\n",
    "   - Train a weak learner (commonly a decision tree) on the dataset with the goal of predicting the residuals. The weak learner is usually a shallow tree to maintain simplicity.\n",
    "\n",
    "4. **Calculate Learning Rate and Update Predictions:**\n",
    "   - Multiply the predictions of the weak learner by a small learning rate (to control the step size) and add them to the current predictions. This update adjusts the predictions in the direction that reduces the overall loss.\n",
    "\n",
    "5. **Repeat Steps 2-4:**\n",
    "   - Iterate through steps 2-4 for a predetermined number of iterations or until a stopping criterion is met. In each iteration, a new weak learner is trained on the residuals of the current ensemble.\n",
    "\n",
    "6. **Final Prediction:**\n",
    "   - The final prediction is the sum of the initial prediction and the predictions from all weak learners. The ensemble, built through multiple iterations, creates a robust model capable of capturing complex relationships in the data.\n",
    "\n",
    "The intuition behind the process is that each new weak learner corrects the errors made by the existing ensemble, focusing on the instances that are challenging to predict. The learning process is adaptive, with each learner adjusting its predictions based on the negative gradient of the loss function with respect to the current predictions.\n",
    "\n",
    "The specific implementation details may vary among different gradient boosting frameworks (such as XGBoost, LightGBM, or scikit-learn's GradientBoostingRegressor/GradientBoostingClassifier). However, the fundamental concept of sequential addition of weak learners to correct errors remains consistent across implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "The mathematical intuition behind the Gradient Boosting algorithm involves understanding the optimization process that minimizes a loss function. The key steps can be outlined as follows:\n",
    "\n",
    "1. **Initialize Predictions:**\n",
    "   - Start with an initial prediction, often the mean of the target variable for regression problems or a class probability for classification problems.\n",
    "\n",
    "2. **Compute Residuals:**\n",
    "   - Calculate the residuals by taking the differences between the actual target values and the current predictions. Residuals represent the errors made by the current ensemble.\n",
    "\n",
    "3. **Train Weak Learner:**\n",
    "   - Train a weak learner (commonly a decision tree) on the dataset, with the goal of predicting the residuals. The weak learner is trained to capture the remaining patterns or errors in the data.\n",
    "\n",
    "4. **Calculate Negative Gradient:**\n",
    "   - Compute the negative gradient of the loss function with respect to the current predictions. The negative gradient indicates the direction in which the loss function decreases most rapidly.\n",
    "\n",
    "5. **Update Predictions:**\n",
    "   - Multiply the predictions of the weak learner by a small learning rate (to control the step size) and add them to the current predictions. This update adjusts the predictions in the direction that reduces the overall loss.\n",
    "\n",
    "6. **Repeat Steps 2-5:**\n",
    "   - Iterate through steps 2-5 for a predetermined number of iterations or until a stopping criterion is met. Each iteration introduces a new weak learner that focuses on the residuals of the current ensemble.\n",
    "\n",
    "7. **Final Prediction:**\n",
    "   - The final prediction is the sum of the initial prediction and the predictions from all weak learners. The ensemble, built through multiple iterations, creates a robust model capable of capturing complex relationships in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
